import streamlit as st
from langdetect import detect
import google.generativeai as genai
import json, uuid
from datetime import datetime
from PIL import Image
import base64
from io import BytesIO
import edge_tts
import asyncio
import nest_asyncio
import speech_recognition as sr
import hashlib

nest_asyncio.apply()

API_KEY = "AIzaSyB_ei3R2CM7DRwYyZw3YkcPtTXhDe6vH14"
genai.configure(api_key=API_KEY)
model = genai.GenerativeModel("gemini-2.5-flash")

CONVERSATIONS_FILE = "conversations.json"

def load_conversations():
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ù…Ù† Ù…Ù„Ù JSON"""
    try:
        with open(CONVERSATIONS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return {}

def save_conversations(conversations):
    """Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª ÙÙŠ Ù…Ù„Ù JSON"""
    with open(CONVERSATIONS_FILE, "w", encoding="utf-8") as f:
        json.dump(conversations, f, ensure_ascii=False, indent=4)

def detect_language(text):
    """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù„ØºØ© (Ø¹Ø±Ø¨ÙŠ Ø£Ùˆ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ)"""
    try:
        return "ar" if detect(text).startswith("ar") else "en"
    except:
        return "ar"

def get_image_hash(image):
    """ØªÙˆÙ„ÙŠØ¯ Ù…Ø¹Ø±Ù ÙØ±ÙŠØ¯ Ù„Ù„ØµÙˆØ±Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆØ§Ù‡Ø§"""
    buffered = BytesIO()
    image.save(buffered, format="JPEG")
    return hashlib.md5(buffered.getvalue()).hexdigest()

def analyze_image(image):
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini API"""
    with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©..."):
        buffered = BytesIO()
        image.save(buffered, format="JPEG")
        img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
        prompt = "Ø­Ù„Ù„ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙˆØ±Ø© ÙˆØ±Ø¯ Ø¨Ø´ÙƒÙ„ ÙˆØ¯ÙŠ ÙˆØ·Ø¨ÙŠØ¹ÙŠ."
        response = model.generate_content([prompt, {"mime_type": "image/jpeg", "data": img_str}])
        return response.text.strip() if response.text else "Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©."

def generate_response(user_message, chat_id, conversations, image=None, image_hash=None):
    """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø§Ù„Ø¨ÙˆØª Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„ØªØ§Ø±ÙŠØ®"""
    history = conversations.get(chat_id, {"messages": []})["messages"]

    if image and image_hash:
        for msg in history[-2:]:
            if msg.get("role") == "user" and msg.get("input") == "ğŸ“· ØµÙˆØ±Ø© Ù…Ø±ÙÙˆØ¹Ø©" and msg.get("image_hash") == image_hash:
                return None  
    context = "\n".join([
        f"User: {m.get('input', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')}\nBot: {m.get('output', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')}"
        for m in history[-3:] if m.get('input') and m.get('output')
    ])

    with st.spinner("Ø§Ù„Ø¨ÙˆØª Ø¨ÙŠÙÙƒØ±..."):
        if image:
            bot_reply = analyze_image(image)
        else:
            lang = detect_language(user_message) if user_message else "ar"
            prompt = (
                f"Language: {lang}\n"
                f"History:\n{context}\n"
                f"User: {user_message}\n"
                f"Ø±Ø¯ ÙˆØªÙÙ‡Ù… Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØªØ­Ù„Ù„Ù‡Ø§ ÙƒÙ…Ø§Ù† ÙˆØªØ±Ø¯ Ø¹Ù„Ù‰ Ø§Ø³Ø§Ø³Ù‡Ø§ Ø·Ø¨ÙŠØ¹ÙŠ ÙˆØªØ¹Ø§Ø·ÙÙŠ ÙƒØ£Ù†Ùƒ ØµØ¯ÙŠÙ‚ Ù…Ù‚Ø±Ø¨."
            )
            response = model.generate_content(prompt)
            bot_reply = response.text.strip() if response.text else "Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯."

    history.append({
        "role": "user",
        "input": user_message if user_message else "ğŸ“· ØµÙˆØ±Ø© Ù…Ø±ÙÙˆØ¹Ø©",
        "timestamp": datetime.now().isoformat(),
        "image_hash": image_hash if image else None
    })
    history.append({
        "role": "assistant",
        "output": bot_reply,
        "timestamp": datetime.now().isoformat()
    })

    conversations[chat_id]["messages"] = history
    conversations[chat_id]["last_updated"] = datetime.now().isoformat()
    save_conversations(conversations)
    return bot_reply

async def text_to_speech(text, filename="output.mp3"):
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Edge TTS"""
    communicate = edge_tts.Communicate(text, voice="ar-EG-SalmaNeural")
    await communicate.save(filename)
    return filename

def play_tts(text):
    """ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª"""
    if not text:
        return
    filename = "output.mp3"
    asyncio.run(text_to_speech(text, filename))
    audio_file = open(filename, "rb").read()
    st.audio(audio_file, format="audio/mp3")

def speech_to_text():
    """Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ Ù†Øµ"""
    recognizer = sr.Recognizer()
    try:
        with sr.Microphone() as source:
            st.session_state.recording_status = "Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„... (Ø§Ø¶ØºØ· 'Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡)"
            st.info("ğŸ¤ Ø§ØªÙƒÙ„Ù… Ø¯Ù„ÙˆÙ‚ØªÙŠ...")
            recognizer.adjust_for_ambient_noise(source, duration=1)  
            audio = recognizer.listen(source, timeout=None, phrase_time_limit=None)
            with st.spinner("Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª..."):
                text = recognizer.recognize_google(audio, language="ar-EG")
            return text
    except sr.UnknownValueError:
        return "Ù…Ø§ÙÙ‡Ù…ØªØ´ Ø§Ù„ØµÙˆØªØŒ Ø¬Ø±Ø¨ ØªØªÙƒÙ„Ù… Ø¨ÙˆØ¶ÙˆØ­ Ø£ÙƒØªØ±."
    except sr.RequestError as e:
        return f"ÙÙŠ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØªØŒ ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª: {str(e)}"
    except sr.WaitTimeoutError:
        return "Ù…Ø§ Ø³Ø¬Ù„ØªØ´ ØµÙˆØªØŒ Ø¬Ø±Ø¨ ØªØ§Ù†ÙŠ."
    except Exception as e:
        return f"Ø­ØµÙ„ Ø®Ø·Ø£: {str(e)}. ØªØ£ÙƒØ¯ Ø¥Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ù…ØªÙˆØµÙ„ ÙˆØ´ØºØ§Ù„."

def start_recording():
    """Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„"""
    if not st.session_state.get("is_recording", False):
        st.session_state.is_recording = True
        st.session_state.voice_text = None
        try:
            mic = sr.Microphone()
            mic.__enter__()  
            mic.__exit__(None, None, None)  
            st.session_state.voice_text = speech_to_text()
        except Exception as e:
            st.session_state.recording_status = f"ÙØ´Ù„ Ø§Ù„ØªØ³Ø¬ÙŠÙ„: {str(e)}. ØªØ£ÙƒØ¯ Ø¥Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ù…ØªÙˆØµÙ„."
            st.session_state.is_recording = False

def stop_recording(response_container, chat_id, conversations):
    """Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ ÙÙˆØ±Ù‹Ø§"""
    if st.session_state.get("is_recording", False):
        st.session_state.is_recording = False
        voice_text = st.session_state.get("voice_text", None)
        if voice_text and not voice_text.startswith(("Ù…Ø§ÙÙ‡Ù…ØªØ´", "ÙÙŠ Ù…Ø´ÙƒÙ„Ø©", "Ø­ØµÙ„ Ø®Ø·Ø£", "Ù…Ø§ Ø³Ø¬Ù„ØªØ´")):
            st.session_state.recording_status = "ØªÙ… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"
            with response_container.container():
                with st.chat_message("user", avatar="ğŸ‘¤"):
                    st.markdown(voice_text)
            response = generate_response(voice_text, chat_id, conversations)
            if response:
                with response_container.container():
                    with st.chat_message("assistant", avatar="ğŸ¤–"):
                        st.markdown(response)
                        play_tts(response)
        else:
            st.session_state.recording_status = voice_text or "ÙØ´Ù„ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ØŒ Ø¬Ø±Ø¨ ØªØ§Ù†ÙŠ."
            st.error(st.session_state.recording_status)
        st.session_state.voice_text = None

def run_app():
    """ØªØ´ØºÙŠÙ„ ØªØ·Ø¨ÙŠÙ‚ Streamlit"""
    st.set_page_config(page_title="EmpathyBot", page_icon="ğŸ¤–", layout="wide")
    st.title("ğŸ¤– EmpathyBot ")

    conversations = load_conversations()
    if "chat_id" not in st.session_state:
        st.session_state.chat_id = str(uuid.uuid4())
        conversations[st.session_state.chat_id] = {
            "messages": [],
            "created_at": datetime.now().isoformat(),
            "title": "New Chat"
        }
        save_conversations(conversations)

    if "last_image_hash" not in st.session_state:
        st.session_state.last_image_hash = None
    if "uploaded_file" not in st.session_state:
        st.session_state.uploaded_file = None
    if "is_recording" not in st.session_state:
        st.session_state.is_recording = False
    if "recording_status" not in st.session_state:
        st.session_state.recording_status = "ØºÙŠØ± Ù…Ø³Ø¬Ù„"
    if "voice_text" not in st.session_state:
        st.session_state.voice_text = None

    messages = conversations[st.session_state.chat_id]["messages"]

    response_container = st.empty()

    for i, msg in enumerate(messages):
        if msg.get("role") == "user":
            with st.chat_message("user", avatar="ğŸ‘¤"):
                st.markdown(msg.get("input", "ØºÙŠØ± Ù…ØªÙˆÙØ±"))
        elif msg.get("role") == "assistant":
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                st.markdown(msg.get("output", "ØºÙŠØ± Ù…ØªÙˆÙØ±"))
                if i == len(messages) - 1:
                    play_tts(msg.get("output"))

    st.write(f"Ø­Ø§Ù„Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„: {st.session_state.recording_status}")

    col1, col2, col3 = st.columns([6, 1, 1])
    with col1:
        user_msg = st.chat_input("Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ Ù‡Ù†Ø§...")
    with col2:
        uploaded = st.file_uploader("ğŸ“", type=["jpg", "jpeg", "png"], label_visibility="collapsed", key="file_uploader")
        st.session_state.uploaded_file = uploaded
    with col3:
        if st.session_state.is_recording:
            if st.button("ğŸ›‘ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„"):
                stop_recording(response_container, st.session_state.chat_id, conversations)
                st.rerun()
        else:
            if st.button("ğŸ¤ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„"):
                start_recording()
                st.rerun()

    if user_msg:
        with response_container.container():
            with st.chat_message("user", avatar="ğŸ‘¤"):
                st.markdown(user_msg)
        response = generate_response(user_msg, st.session_state.chat_id, conversations)
        if response:
            with response_container.container():
                with st.chat_message("assistant", avatar="ğŸ¤–"):
                    st.markdown(response)
                    play_tts(response)
        st.rerun()

    if st.session_state.uploaded_file:
        image = Image.open(st.session_state.uploaded_file)
        image_hash = get_image_hash(image)
        if image_hash != st.session_state.last_image_hash:
            response = generate_response("", st.session_state.chat_id, conversations, image=image, image_hash=image_hash)
            if response:
                with response_container.container():
                    with st.chat_message("user", avatar="ğŸ‘¤"):
                        st.markdown("ğŸ“· ØµÙˆØ±Ø© Ù…Ø±ÙÙˆØ¹Ø©")
                    with st.chat_message("assistant", avatar="ğŸ¤–"):
                        st.markdown(response)
                        play_tts(response)
                st.session_state.last_image_hash = image_hash
                st.session_state.uploaded_file = None
                st.success("ØªÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© ÙˆØ¥Ø²Ø§Ù„ØªÙ‡Ø§ Ù…Ù† Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©!")
                st.rerun()

if __name__ == "__main__":
    run_app()
